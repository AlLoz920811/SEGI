# PDF Split & Extract API (FastAPI)

API para **dividir PDFs por p√°ginas**, **extraer contenido** (Agentic Document Extraction), **generar tablas estructuradas con LLM** y **cargar resultados a PostgreSQL**.  
Proyecto compuesto por:

- `main.py` ‚Üí API con FastAPI
- `helpers.py` ‚Üí utilidades (validaci√≥n de archivos, limpieza, funciones LLM, etc.)

> Nota: si en tu repo el archivo se llama `helper.py` en singular, ren√≥mbralo a `helpers.py` o ajusta los `import`.

---

## üöÄ Caracter√≠sticas

- **/split**: valida PDF y separa por p√°ginas en `./pages`.
- **/extract**: analiza una p√°gina PDF (`pages/<base>_page_<N>.pdf`), construye un DataFrame con *chunks* y lo guarda como Excel en `./results`.
- **/generate**: toma un Excel de `./results`, resume/estructura con LLM y guarda la tabla final en `./tables`.
- **/insert**: lee un Excel de `./tables` y lo inserta en PostgreSQL (`tbl_captura_ia`) con `pg8000`.

Estructura de carpetas (se crean al iniciar la app):
```
files/    # PDFs de entrada
pages/    # PDFs paginados (<base>_page_<N>.pdf)
results/  # extracci√≥n cruda a Excel
tables/   # tablas generadas (listas "aplanadas") a Excel
```

---

## üß© Endpoints

| M√©todo | Ruta        | Descripci√≥n |
|---|---|---|
| GET | `/` | Mensaje de bienvenida. |
| GET | `/split?filename=<archivo.pdf>` | Valida y separa el PDF por p√°ginas en `pages/`. |
| GET | `/extract?filename=<base>_page_<N>.pdf` | Extrae contenido (chunks) y guarda Excel en `results/`. |
| GET | `/generate?filename=<base>_page_<N>.xlsx` | LLM ‚Üí JSON ‚Üí DataFrame; enriquece metadatos y guarda Excel en `tables/`. |
| GET | `/insert?filename=<base>_page_<N>_generated.xlsx` | Inserta el Excel de `tables/` a PostgreSQL (`tbl_captura_ia`). |

### Ejemplos r√°pidos

- Split:  
  `GET http://localhost:8000/split?filename=covalca_3.pdf`
- Extract:  
  `GET http://localhost:8000/extract?filename=covalca_3_page_16.pdf`
- Generate:  
  `GET http://localhost:8000/generate?filename=covalca_1_page_1.xlsx`
- Insert:  
  `GET http://localhost:8000/insert?filename=covalca_1_page_1_generated.xlsx`

> Abre `http://localhost:8000/docs` para probar con Swagger.

---

## ‚öôÔ∏è Configuraci√≥n

La API requiere ciertas variables de entorno para funcionar correctamente. Estas se pueden definir en un archivo `.env` en la ra√≠z del proyecto. El archivo `.env` est√° ignorado por Git para proteger las credenciales.

Un ejemplo de `.env`:

```shell
# Azure Application Insights (opcional)
APPINSIGHTS_CONNECTION_STRING="tu_connection_string_de_app_insights"

# Credenciales de servicios
VISION_AGENT_API_KEY="tu_vision_agent_api_key"
OPENAI_API_KEY="tu_openai_api_key"

# Configuraci√≥n de la base de datos PostgreSQL
DB_HOST="tu_db_host"
DB_PORT="5432"
DB_NAME="tu_db_name"
DB_USER="tu_db_user"
DB_PASSWORD="tu_db_password"
```

### Logging

La aplicaci√≥n integra logging con **Azure Application Insights**. Si se provee la variable `APPINSIGHTS_CONNECTION_STRING`, los logs de la aplicaci√≥n (eventos de ciclo de vida, peticiones a endpoints y errores) ser√°n enviados a Azure. Si la variable no se encuentra, el logging se realizar√° en la consola, permitiendo el desarrollo y la depuraci√≥n en local sin necesidad de una conexi√≥n a Azure.

---

## üõ†Ô∏è Dependencias

- **FastAPI**, **Uvicorn** (ASGI)
- **pypdf** (o PyPDF2 como respaldo)
- **pandas**, **openpyxl**
- **pg8000** (PostgreSQL)
- **python-dotenv** (cargar `.env`)
- **beautifulsoup4** (parseo de tabla HTML)
- Paquete del extractor: `agentic_doc.parse` (tu framework de **Agentic Document Extraction**)
- **openai** (cliente Python para la etapa LLM)
- **opencensus-ext-azure** (logging en Azure Application Insights)

Instalaci√≥n t√≠pica:

```bash
pip install fastapi uvicorn pypdf pandas openpyxl pg8000 python-dotenv beautifulsoup4 openai opencensus-ext-azure
# y tu paquete/SDK para agentic_doc.parse
```

Python recomendado: **3.10+**.

---

## üîê Variables de entorno

Crea un archivo `.env` en la ra√≠z del proyecto:

```env
# OpenAI
OPENAI_API_KEY=tu_api_key
VISION_AGENT_API_KEY=tu_api_key_opcional

# PostgreSQL
DB_HOST=localhost
DB_PORT=5432
DB_NAME=mi_base
DB_USER=mi_usuario
DB_PASSWORD=mi_password

# Azure Application Insights (opcional)
APPINSIGHTS_CONNECTION_STRING=tu_connection_string_de_app_insights
```

`helpers.get_secret` carga estos valores con soporte para `.env`.

---

## ‚ñ∂Ô∏è C√≥mo ejecutar

1) Clona e instala dependencias.  
2) Ejecuta la API:

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

- En el navegador usa **http://localhost:8000** (no `0.0.0.0`).
- Documentaci√≥n interactiva: **http://localhost:8000/docs**

---

## üß† Flujo de trabajo

1. **/split**  
   - Verifica `.pdf` / `.PDF`.  
   - Normaliza extensi√≥n a `.pdf` (maneja *case-insensitive* de Windows).  
   - Divide por p√°ginas: `pages/<base>_page_<N>.pdf`.

2. **/extract**  
   - Usa `agentic_doc.parse.parse(...)`.  
   - Construye un DataFrame con:
     - `chunk_id`, `chunk_type`, `text_html`  
     - Metadatos: `name_file`, `url_file`, `page`, `active`, `capture_log`, `subject_mail`  
     - `clean_text` (mediante `parse_table_replace`)  
   - Guarda como Excel en `results/`.

3. **/generate**  
   - Lee el Excel de `results`.  
   - Concatena `clean_text` ‚Üí prompt a LLM:
     - `generate_invoice_json` ‚Üí `_extract_json_from_text` ‚Üí `balance_lists_by_item_id`  
   - Crea `gen_df` (una fila por item).  
   - `enrich_df(df, gen_df)` copia metadatos desde `df`.  
   - Guarda como Excel en `tables/` (`*_generated.xlsx`).

4. **/insert**  
   - Valida `.xlsx` (`ensure_xlsx_extension`).  
   - Lee `tables/*.xlsx` a `df` y reemplaza vac√≠os por `"NULL"` (literal).  
   - Renombra columnas si aplica (`item_id`‚Üí`item`, `page`‚Üí`page_number`).  
   - Inserta en **PostgreSQL** tabla `tbl_captura_ia` con `pg8000`.

---

## üß∞ Helpers destacados

- **Validaci√≥n/normalizaci√≥n de archivos**
  - `ensure_xlsx_extension(path)`  
  - `_ensure_lowercase_pdf_extension(path)`  
  - `preprocess_filename(filename, files_dir)`
- **PDF**
  - `split_pdf_to_pages(input_pdf, output_dir)`
  - `extract_page_number(filename)`
  - `extract_original_pdf_name(file_name)`
- **Parsing/Limpieza**
  - `html_table_to_tuples(html)`  
  - `parse_table_replace(row)`  
  - `clean_filename(filename)`
- **LLM**
  - `generate_invoice_json(client, resume_markdown)`  
  - `_extract_json_from_text(text)`  
  - `balance_lists_by_item_id(dict)`  
  - `enrich_df(df, gen_df)`  
- **Secrets**
  - `get_secret(var_name, dotenv_path=None, ...)`

---

## ‚ö†Ô∏è Errores comunes y tips

- **422 (Unprocessable Content)**: par√°metro incorrecto. Ej.: `/extract` requiere `filename` (no `file_name`).  
- **415 (Unsupported Media Type)**: extensi√≥n inv√°lida.  
- **404**: archivo no encontrado en la carpeta esperada.  
- **0.0.0.0** no es navegable: usa `http://localhost:8000` o `http://127.0.0.1:8000`.  
- Al insertar en PostgreSQL, la cadena `"NULL"` es **texto**, no `NULL` SQL. Si necesitas `NULL` real, usa `None` en lugar de `"NULL"`.

---

## üìÇ Estructura sugerida del repo

```
.
‚îú‚îÄ main.py
‚îú‚îÄ helpers.py
‚îú‚îÄ files/
‚îú‚îÄ pages/
‚îú‚îÄ results/
‚îú‚îÄ tables/
‚îú‚îÄ .env.example
‚îî‚îÄ README.md
```

`./.env.example` (opcional) con las variables de entorno vac√≠as para gu√≠a.

---

## üß™ Pruebas r√°pidas (curl)

```bash
# Split
curl "http://localhost:8000/split?filename=covalca_3.pdf"

# Extract
curl "http://localhost:8000/extract?filename=covalca_3_page_16.pdf"

# Generate
curl "http://localhost:8000/generate?filename=covalca_1_page_1.xlsx"

# Insert
curl "http://localhost:8000/insert?filename=covalca_1_page_1_generated.xlsx"
```

---

## üìú Licencia

MIT (o la que prefieras). A√±ade `LICENSE` al repo.

---

## ü§ù Contribuciones

Issues y PRs son bienvenidos.  
Por favor incluye:
- Descripci√≥n clara del cambio
- Pasos para reproducir
- Ejemplos de entrada/salida si aplica
